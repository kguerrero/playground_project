{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74080c68-b26a-45c5-9ac9-04d3e167d1b3",
   "metadata": {},
   "source": [
    "# Inspirational Developer Helper: A RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0133ce7f-5633-404b-a9d8-d16113768843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MultilineStringFormatter at 0x780c954e3770>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "import faiss\n",
    "from huggingface_hub import InferenceClient\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from IPython.core.formatters import BaseFormatter\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "class MultilineStringFormatter(BaseFormatter):\n",
    "    def __call__(self, obj):\n",
    "        if isinstance(obj, str) and '\\n' in obj:\n",
    "            return f'<pre>{obj}</pre>'\n",
    "        return None\n",
    "\n",
    "# Register the custom formatter\n",
    "ip = get_ipython()\n",
    "ip.display_formatter.formatters['text/html'].for_type(str, MultilineStringFormatter())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba981df-3e05-498d-93a8-8e1a98d36038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up FAISS index\n",
    "\n",
    "def prep_documents(pdf_path: str) -> List[Document]:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "    return chunks\n",
    "    \n",
    "# would need to change these to match your local file system store for data\n",
    "pdf_path = \"../data/raw_documents/The Pragmatic Programmer, 20th Anniversary Edition.pdf\"\n",
    "index_path = \"../data/faiss_index\"\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "docs = prep_documents(pdf_path)\n",
    "vectorstore = FAISS.from_documents(\n",
    "    docs, \n",
    "    embedding_function\n",
    ")\n",
    "\n",
    "# Save the vector store\n",
    "vectorstore.save_local(index_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "612c8eb2-e070-4428-b478-a7abdefc0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load existing faiss index\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "       index_path, embedding_function, allow_dangerous_deserialization=True\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "719c973b-8a20-4c54-83f8-3e18a9c8ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\"\n",
    "\n",
    "q = \"\"\"What is the best choice for a developer to get reinspired in their career\"\"\"\n",
    "\n",
    "# retrieve context\n",
    "\n",
    "retrieved_docs = vectorstore.similarity_search(query=q, k=5)\n",
    "retrieved_content = [(d.page_content) for d in retrieved_docs]\n",
    "context = \"\\n\".join([r for r in retrieved_content])\n",
    "prompt = PROMPT_TEMPLATE.format(context=context, question=q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c78d3a6-96a8-45c8-b185-3b808a0c9de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('According to the text, the best choice for a developer who feels stuck in '\n",
      " 'their career is to \"try to fix it\" by making a change, rather than simply '\n",
      " 'hoping things will get better. This can involve changing their organization, '\n",
      " 'learning new skills, or seeking out new opportunities. The text suggests '\n",
      " 'that developers have agency and control over their careers, and that they '\n",
      " 'should take proactive steps to address their frustrations and create the '\n",
      " 'career they want.\\n'\n",
      " '\\n'\n",
      " 'Specifically, the text suggests the following options:\\n'\n",
      " '\\n'\n",
      " \"* Try a sophisticated IDE with cutting-edge features if you've only used \"\n",
      " 'makefiles and an editor\\n'\n",
      " \"* Read news and posts online on technology different from what you're \"\n",
      " 'currently working with\\n'\n",
      " '* Take classes or participate in local user groups and meetups to learn new '\n",
      " 'skills and stay current\\n'\n",
      " '* Experiment with different environments, such as switching from Windows to '\n",
      " 'Linux\\n'\n",
      " '* Seek out new opportunities, such as working remotely or moving to a new '\n",
      " 'location\\n'\n",
      " '\\n'\n",
      " \"Overall, the text emphasizes the importance of taking control of one's \"\n",
      " 'career and making intentional choices to create a fulfilling and challenging '\n",
      " 'work life.')\n"
     ]
    }
   ],
   "source": [
    "# remote llm\n",
    "remote_chat_completion_llm = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llm_client = InferenceClient(remote_chat_completion_llm)\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d983c2ed-b4e7-41d1-8320-b24ced0c94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('According to the provided text, when a developer feels stuck in their '\n",
      " 'career, the best choice is to **\"Why canâ€™t you change it?\"** This means '\n",
      " 'being proactive and taking control of the situation. The text suggests '\n",
      " 'several ways to do this:\\n'\n",
      " '\\n'\n",
      " '*   **Fix it:** If your work environment is unsatisfactory, try to improve '\n",
      " 'it.\\n'\n",
      " '*   **Invest in yourself:** Dedicate time to learn new technologies and stay '\n",
      " 'current, even outside of work hours.\\n'\n",
      " '*   **Seek new opportunities:** If necessary, find a new job with a better '\n",
      " 'environment or explore remote work options.\\n'\n",
      " '*   **Take courses:** Enroll in classes to gain new skills.\\n'\n",
      " '*   **Participate in user groups and meetups:** Network with other '\n",
      " 'developers to stay informed and avoid isolation.\\n'\n",
      " '*   **Experiment with different environments:** Try new technologies and '\n",
      " 'approaches to broaden your skillset.\\n'\n",
      " '\\n'\n",
      " \"The core message is that developers have agency and shouldn't passively \"\n",
      " 'accept a dissatisfying career path. They have the power to create the career '\n",
      " 'they want.')\n"
     ]
    }
   ],
   "source": [
    "# call model with context: local\n",
    "LOCAL_LLM_MODEL = os.getenv('LLM_MODEL', 'gemma3n')\n",
    "local_OLLAMA_HOST = os.getenv('OLLAMA_HOST', 'http://localhost:11434')\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "llm_client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "\n",
    "response = llm_client.chat.completions.create(\n",
    "  model=LOCAL_LLM_MODEL,\n",
    "    messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    ")\n",
    "pprint(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22eb672-0e27-4a5e-ad51-61b303d06990",
   "metadata": {},
   "source": [
    "# Potential Next Steps\n",
    "1. Evaluation\n",
    "\n",
    "    a. Build an evaluation dataset for what answers are good enough\n",
    "\n",
    "   \n",
    "    b. Set up tracking on runtime and memory usage\n",
    "\n",
    "   \n",
    "    c. Evaluate other model and parameter choices to keep baseline performance and optimize on memory and time costs\n",
    "\n",
    "2. In particular I would like to play around with CoRAG\n",
    "\n",
    "3. Move from a notebook interface into files and functions and a flask app\n",
    "\n",
    "4. Wrap everything using smolagents and add some tooling and routing to build out a little agent\n",
    "\n",
    "1. Evaluation\n",
    "\n",
    "    a. Build an evaluation dataset for what answers are good enough\n",
    "\n",
    "   \n",
    "    b. Set up tracking on runtime and memory usage\n",
    "\n",
    "   \n",
    "    c. Evaluate other model and parameter choices to keep baseline performance and optimize on memory and time costs\n",
    "\n",
    "         In particular I would like to play around with CoRAG and some complicated asks\n",
    "\n",
    "3. Move from a notebook interface into files and functions and a flask app\n",
    "\n",
    "4. Wrap everything using smolagents and add some tooling and routing to build out a little agent\n",
    "\n",
    "5. Use langchain to cleanup the manual steps\n",
    "\n",
    "6. Create a security harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ceda7a-526d-4fab-ae5f-bf373661ca89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
